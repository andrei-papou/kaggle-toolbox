{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasty fix to make notebook load the library without pip-installing it.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from kaggle_toolbox import device\n",
    "from kaggle_toolbox.data import DatasetItem, DatasetItemCollator\n",
    "from kaggle_toolbox.device import CUDADevice\n",
    "from kaggle_toolbox.features.transform import contiguous_to_categorical\n",
    "from kaggle_toolbox.logging import StdOutLogger\n",
    "from kaggle_toolbox.loss.regression import MSELoss\n",
    "from kaggle_toolbox.lr_scheduling import create_cosine_scheduler_with_warmup\n",
    "from kaggle_toolbox.metrics.regression import MCRMSEMetric\n",
    "from kaggle_toolbox.nlp.transformer import Backbone, StandardModel, ClsTokenPooler, TakeNthHiddenLayerOutputSqueezer, \\\n",
    "    create_nakama_optimizer, get_tokenizer_for_backbone, Tokenizer, TokenizerResult\n",
    "from kaggle_toolbox.oof import OOFPredDict\n",
    "from kaggle_toolbox.path import format_path\n",
    "from kaggle_toolbox.trainer import StandardIterationTrainer, FullCycleTrainer, train_kfold_model\n",
    "from kaggle_toolbox.typing import DynamicDict\n",
    "from kaggle_toolbox.validation import analyze_val_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(TorchDataset[DatasetItem[TokenizerResult]]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            tokenizer: Tokenizer,\n",
    "            max_len: int,\n",
    "            target_list: t.List[str]):\n",
    "        self._df = df.copy().reset_index(drop=True)\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._target_list = target_list\n",
    "\n",
    "    def _get_tokenizer_input(self, row: DynamicDict) -> str:\n",
    "        (\n",
    "            full_text,\n",
    "         ) = (\n",
    "            row.get_typed_or_raise('full_text', str),\n",
    "         )\n",
    "\n",
    "        return full_text\n",
    "\n",
    "    def sort_by_tokenizer_input_len(self):\n",
    "        self._df['_tok_input_len'] = self._df.progress_apply(self._get_tokenizer_input, axis=1)\n",
    "        self._df = self._df.sort_values('_tok_input_len')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> DatasetItem[TokenizerResult]:\n",
    "        row = self._df.iloc[idx]\n",
    "\n",
    "        tokenizer_input = self._get_tokenizer_input(DynamicDict(t.cast(t.Dict[str, t.Any], row)))\n",
    "        id = str(row['text_id'])\n",
    "\n",
    "        tokenizer_result = self._tokenizer.tokenize(\n",
    "            tokenizer_input, max_len=self._max_len)\n",
    "        target_tensor = torch.tensor(\n",
    "            [float(row[target]) for target in self._target_list],\n",
    "            dtype=torch.float32)\n",
    "\n",
    "        return DatasetItem(\n",
    "            id=[id],\n",
    "            x=tokenizer_result,\n",
    "            y=target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LIST = [\n",
    "    'cohesion',\n",
    "    'syntax',\n",
    "    'vocabulary',\n",
    "    'phraseology',\n",
    "    'grammar',\n",
    "    'conventions',\n",
    "]\n",
    "\n",
    "SEED = 42\n",
    "NUM_FOLDS = 5\n",
    "DEVICE = CUDADevice()\n",
    "BACKBONE = 'microsoft/deberta-v3-small'\n",
    "MAX_LEN = 1024\n",
    "ENCODER_LR = 1e-5\n",
    "DECODER_LR = 1e-4\n",
    "BATCH_SIZE = 2\n",
    "ACCUMULATE_GRADIENT_STEPS = 1\n",
    "NUM_EPOCHS = 3\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "ROOT_DIR = Path('/kaggle')\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "FP_ELL_DATASET_DIR = DATA_DIR / 'fp-ell'\n",
    "MODEL_DIR = ROOT_DIR / 'models'\n",
    "OOF_DIR = ROOT_DIR / 'oof'\n",
    "\n",
    "RUN_ID = 'v1-test'\n",
    "MODEL_PATH_TEMPLATE = MODEL_DIR / f'{RUN_ID}-fold_{{fold}}.pt'\n",
    "OOF_PATH = OOF_DIR / f'{RUN_ID}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_data(dataset_dir_path: Path, target_list: t.List[str], num_folds: int, seed: int) -> pd.DataFrame:\n",
    "    all_df = pd.read_csv(dataset_dir_path / 'train.csv')\n",
    "    target_arr = contiguous_to_categorical(all_df[target_list].values)\n",
    "\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "    for fold_, (_, v_) in enumerate(mskf.split(X=all_df, y=target_arr)):\n",
    "        all_df.loc[v_, 'fold'] = fold_\n",
    "\n",
    "    return all_df\n",
    "\n",
    "all_df = _read_data(\n",
    "    dataset_dir_path=FP_ELL_DATASET_DIR,\n",
    "    target_list=TARGET_LIST,\n",
    "    num_folds=NUM_FOLDS,\n",
    "    seed=SEED)\n",
    "\n",
    "analyze_val_strategy(all_df, target_list=TARGET_LIST, num_folds=NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model(fold: int) -> t.Tuple[float, OOFPredDict]:\n",
    "    backbone = Backbone.from_huggingface_checkpoint(BACKBONE, zero_out_dropout=True)\n",
    "    tokenizer = get_tokenizer_for_backbone(backbone=BACKBONE)\n",
    "    model: StandardModel[TokenizerResult] = StandardModel(\n",
    "        backbone=backbone,\n",
    "        pooler=ClsTokenPooler(),\n",
    "        squeezer=TakeNthHiddenLayerOutputSqueezer(),\n",
    "        dnn=torch.nn.Sequential(\n",
    "            torch.nn.Linear(backbone.out_dim_size, len(TARGET_LIST))))\n",
    "    optimizer = create_nakama_optimizer(\n",
    "        model=model,\n",
    "        encoder_lr=ENCODER_LR,\n",
    "        decoder_lr=DECODER_LR)\n",
    "    trainer: FullCycleTrainer[TokenizerResult] = FullCycleTrainer(\n",
    "        iteration_trainer=StandardIterationTrainer(\n",
    "            model=model,\n",
    "            criterion=MSELoss(),\n",
    "            optimizer=optimizer,\n",
    "            scheduler=create_cosine_scheduler_with_warmup(\n",
    "                optimizer=optimizer,\n",
    "                num_training_steps=1,\n",
    "                warmup_steps_ratio=0.0,\n",
    "                num_cycles=0.5),\n",
    "            pred_quality_metric_list=[\n",
    "                MCRMSEMetric(),\n",
    "            ],\n",
    "            device=DEVICE,\n",
    "            accumulate_gradient_steps=ACCUMULATE_GRADIENT_STEPS),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collator=DatasetItemCollator(x_collate_fn=tokenizer.result_type.collate_fn),\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        model_comparison_metric=MCRMSEMetric.valid_name,\n",
    "        model_comparison_metric_criteria=MCRMSEMetric.criteria,\n",
    "        save_model_to_path=format_path(MODEL_PATH_TEMPLATE, fold=str(fold)),\n",
    "        logger_list=[\n",
    "            StdOutLogger(),\n",
    "        ])\n",
    "\n",
    "    train_df, valid_df = all_df[all_df['fold'] != fold], all_df[all_df['fold'] == fold]\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        target_list=TARGET_LIST)\n",
    "    valid_dataset = Dataset(\n",
    "        df=valid_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        target_list=TARGET_LIST)\n",
    "\n",
    "    return trainer.do_full_cycle(train_dataset, valid_dataset)\n",
    "\n",
    "score_list, oof_pred_dict = train_kfold_model(\n",
    "    train_model_fn=_train_model,\n",
    "    fold_list=list(range(NUM_FOLDS)))\n",
    "oof_pred_dict.save_to_csv(\n",
    "    OOF_PATH,\n",
    "    score_col_name_list=[f'{target}_score' for target in TARGET_LIST])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('kaggle-toolbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8b57a415e74ffcd40c77067d54e79fe9904e4ff55eaf76135a23be1f98d14e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
